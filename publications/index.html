<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title> Haozhe Ji | publications</title>
    <meta name="author" content=" Haozhe Ji" />
    <meta name="description" content="<code>*</code> means equal contribution" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒ‘</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://cdjhz.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://cdjhz.github.io/"><span class="font-weight-bold"></span> Haozhe  Ji</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">awards</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">services</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"><code>*</code> means equal contribution</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">Preprint</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Arxiv</abbr></div>

        <!-- Entry bib key -->
        <div id="ji2023language" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Language Model Decoding as Direct Metrics Optimization</div>
          <!-- Author -->
          <div class="author">
                <strong><u>Haozhe Ji</u></strong>,Â Pei Ke,Â Hongning Wang,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em></em> <br>Arxiv Preprint
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://arxiv.org/abs/2310.01041" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://arxiv.org/pdf/2310.01041.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. In particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. Overall, these methods fall short in achieving holistic alignment across a broad range of aspects. In this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. The resulting decoding distribution enjoys an analytical solution that scales the input language model distribution via a sequence-level energy function defined by these metrics. And most importantly, we prove that this induced distribution is guaranteed to improve the perplexity on human texts, which suggests a better approximation to the underlying distribution of human texts. To facilitate tractable sampling from this globally normalized distribution, we adopt the Sampling-Importance-Resampling technique. Experiments on various domains and model scales demonstrate the superiority of our method in metrics alignment with human texts and human evaluation over strong baselines.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/iclr/JiH23" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Tailoring Language Generation Models under Total Variation Distance</div>
          <!-- Author -->
          <div class="author">
                <strong><u>Haozhe Ji</u></strong>,Â Pei Ke,Â Zhipeng Hu,Â Rongsheng Zhang,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>The Eleventh International Conference on Learning Representations,</em> <br>ICLR 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://openreview.net/forum?id=VELL0PlWfc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://openreview.net/pdf?id=VELL0PlWfc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/thu-coai/TaiLr" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/naacl/JiH22" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">LaMemo: Language Modeling with Look-Ahead Memory</div>
          <!-- Author -->
          <div class="author">
                <strong><u>Haozhe Ji</u></strong>,Â Rongsheng Zhang,Â Zhenyu Yang,Â Zhipeng Hu,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</em> <br>NAACL 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://arxiv.org/abs/2204.07341" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://arxiv.org/abs/2204.07341.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/thu-coai/lamemo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AI Open</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:journals/corr/abs-2012-00413" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">CPM: A Large-scale Generative Chinese Pre-trained Language Model</div>
          <!-- Author -->
          <div class="author">Zhengyan Zhang,Â Xu Han,Â Hao Zhou,Â Pei Ke,Â Yuxian Gu,Â Deming Ye,Â Yujia Qin,Â YuSheng Su,Â 
                <strong><u>Haozhe Ji</u></strong>,Â Jian Guan,Â Fanchao Qi,Â Xiaozhi Wang,Â Yanan Zheng,Â Guoyang Zeng,Â Huanqi Cao,Â Shengqi Chen,Â Daixuan Li,Â Zhenbo Sun,Â Zhiyuan Liu,Â <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>,Â Wentao Han,Â Jie Tang,Â Juanzi Li,Â Xiaoyan Zhu,Â and Maosong Sun
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>AI Open</em> <br>AI Open 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://arxiv.org/abs/2012.00413" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://github.com/TsinghuaAI/CPM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACL Findings</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/acl/KeJRCWSZH21" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">JointGT: Graph-Text Joint Representation Learning for Text Generation
               from Knowledge Graphs</div>
          <!-- Author -->
          <div class="author">Pei Ke,Â 
                <strong><u>Haozhe Ji</u></strong>,Â Yu Ran,Â Xin Cui,Â Liwei Wang,Â Linfeng Song,Â Xiaoyan Zhu,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>Findings of the Association for Computational Linguistics,</em> <br>ACL Findings 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://aclanthology.org/2021.findings-acl.223/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://aclanthology.org/2021.findings-acl.223.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/thu-coai/JointGT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the em- bedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/emnlp/JiH21" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational
               Transformer</div>
          <!-- Author -->
          <div class="author">
                <strong><u>Haozhe Ji</u></strong>,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>Proceedings of the 2021 Conference on Empirical Methods in Natural
               Language Processing,</em> <br>EMNLP 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://aclanthology.org/2021.emnlp-main.347/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://aclanthology.org/2021.emnlp-main.347.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/cdjhz/discodvt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/emnlp/KeJLZH20" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic
               Knowledge</div>
          <!-- Author -->
          <div class="author">Pei Ke*,Â 
                <strong><u>Haozhe Ji*</u></strong>,Â Siyang Liu,Â Xiaoyan Zhu,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>the 2020 Conference on Empirical Methods in Natural
               Language Processing,</em> <br>EMNLP 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://aclanthology.org/2020.emnlp-main.567/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://aclanthology.org/2020.emnlp-main.567.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/thu-coai/SentiLARE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AACL</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/ijcnlp/JiKHWH20" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Generating Commonsense Explanation by Extracting Bridge Concepts from
               Reasoning Paths</div>
          <!-- Author -->
          <div class="author">
                <strong><u>Haozhe Ji</u></strong>,Â Pei Ke,Â Shaohan Huang,Â Furu Wei,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the
               Association for Computational Linguistics,</em> <br>AACL 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://aclanthology.org/2020.aacl-main.28/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://aclanthology.org/2020.aacl-main.28.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Commonsense explanation generation aims to empower the machineâ€™s sense-making capability by generating plausible explanations to statements against commonsense. While this task is easy to human, the machine still struggles to generate reasonable and informative explanations. In this work, we propose a method that first extracts the underlying concepts which are served as bridges in the reasoning chain and then integrates these concepts to generate the final explanation. To facilitate the reasoning process, we utilize external commonsense knowledge to build the connection between a statement and the bridge concepts by extracting and pruning multi-hop paths to build a subgraph. We design a bridge concept extraction model that first scores the triples, routes the paths in the subgraph, and further selects bridge concepts with weak supervision at both the triple level and the concept level. We conduct experiments on the commonsense explanation generation task and our model outperforms the state-of-the-art baselines in both automatic and human evaluation.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/emnlp/JiKHWZH20" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
               Graph</div>
          <!-- Author -->
          <div class="author">
                <strong><u>Haozhe Ji</u></strong>,Â Pei Ke,Â Shaohan Huang,Â Furu Wei,Â Xiaoyan Zhu,Â and <a href="http://coai.cs.tsinghua.edu.cn/hml" target="_blank" rel="noopener noreferrer">Minlie Huang</a>
                  
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing,</em> <br>EMNLP 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://aclanthology.org/2020.emnlp-main.54/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://aclanthology.org/2020.emnlp-main.54.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/cdjhz/multigen" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/acl/SunLLJ18" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Denoising Distantly Supervised Open-Domain Question Answering</div>
          <!-- Author -->
          <div class="author">Yankai Lin,Â 
                <strong><u>Haozhe Ji</u></strong>,Â Zhiyuan Liu,Â and Maosong Sun
          </div>

          <!-- Journal/Book title and date --><div class="periodical">
<em>the 56th Annual Meeting of the Association for Computational
               Linguistics,</em> <br>ACL 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
            <a href="https://aclanthology.org/P18-1161/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">LINK</a>
            <a href="https://aclanthology.org/P18-1161.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/thunlp/OpenQA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2023  Haozhe Ji. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
Last updated: November 18, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

