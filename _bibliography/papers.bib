

@inproceedings{DBLP:conf/acl/SunLLJ18,
  author    = {Yankai Lin and
               Haozhe Ji and
               Zhiyuan Liu and
               Maosong Sun},
  title     = {Denoising Distantly Supervised Open-Domain Question Answering},
  booktitle = {the 56th Annual Meeting of the Association for Computational
               Linguistics,},
  abstract  = {Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.},
  year      = {2018},
  link      = {https://aclanthology.org/P18-1161/},
  pdf       = {https://aclanthology.org/P18-1161.pdf},
  code      = {https://github.com/thunlp/OpenQA},
  abbr      = {ACL},
}

@inproceedings{DBLP:conf/emnlp/KeJLZH20,
  author    = {Pei Ke* and
               Haozhe Ji* and
               Siyang Liu and
               Xiaoyan Zhu and
               Minlie Huang},
  title     = {SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic
               Knowledge},
  booktitle = {the 2020 Conference on Empirical Methods in Natural
               Language Processing,},
  abstract  = {Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.},
  year      = {2020},
  link      = {https://aclanthology.org/2020.emnlp-main.567/},
  pdf       = {https://aclanthology.org/2020.emnlp-main.567.pdf},
  code      = {https://github.com/thu-coai/SentiLARE},
  abbr      = {EMNLP},
}

@inproceedings{DBLP:conf/ijcnlp/JiKHWH20,
  author    = {Haozhe Ji and
               Pei Ke and
               Shaohan Huang and
               Furu Wei and
               Minlie Huang},
  title     = {Generating Commonsense Explanation by Extracting Bridge Concepts from
               Reasoning Paths},
  booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the
               Association for Computational Linguistics,},
  year      = {2020},
  abstract  = {Commonsense explanation generation aims to empower the machineâ€™s sense-making capability by generating plausible explanations to statements against commonsense. While this task is easy to human, the machine still struggles to generate reasonable and informative explanations. In this work, we propose a method that first extracts the underlying concepts which are served as bridges in the reasoning chain and then integrates these concepts to generate the final explanation. To facilitate the reasoning process, we utilize external commonsense knowledge to build the connection between a statement and the bridge concepts by extracting and pruning multi-hop paths to build a subgraph. We design a bridge concept extraction model that first scores the triples, routes the paths in the subgraph, and further selects bridge concepts with weak supervision at both the triple level and the concept level. We conduct experiments on the commonsense explanation generation task and our model outperforms the state-of-the-art baselines in both automatic and human evaluation.},
  link      = {https://aclanthology.org/2020.aacl-main.28/},
  pdf       = {https://aclanthology.org/2020.aacl-main.28.pdf},
  abbr      = {AACL},
}

@inproceedings{DBLP:conf/emnlp/JiKHWZH20,
  author    = {Haozhe Ji and
               Pei Ke and
               Shaohan Huang and
               Furu Wei and
               Xiaoyan Zhu and
               Minlie Huang},
  title     = {Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
               Graph},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing,},
  year      = {2020},
  abstract  = {Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.},
  link      = {https://aclanthology.org/2020.emnlp-main.54/},
  pdf       = {https://aclanthology.org/2020.emnlp-main.54.pdf},  
  abbr      = {EMNLP},
  code      = {https://github.com/cdjhz/multigen},
}

@article{DBLP:journals/corr/abs-2012-00413,
  author    = {Zhengyan Zhang and
               Xu Han and
               Hao Zhou and
               Pei Ke and
               Yuxian Gu and
               Deming Ye and
               Yujia Qin and
               YuSheng Su and
               Haozhe Ji and
               Jian Guan and
               Fanchao Qi and
               Xiaozhi Wang and
               Yanan Zheng and
               Guoyang Zeng and
               Huanqi Cao and
               Shengqi Chen and
               Daixuan Li and
               Zhenbo Sun and
               Zhiyuan Liu and
               Minlie Huang and
               Wentao Han and
               Jie Tang and
               Juanzi Li and
               Xiaoyan Zhu and
               Maosong Sun},
  title     = {{CPM:} {A} Large-scale Generative Chinese Pre-trained Language Model},
  abstract  = {Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning.},
  year      = {2021},
  journal   = {AI Open},
  link      = {https://arxiv.org/abs/2012.00413},
  code      = {https://github.com/TsinghuaAI/CPM},
  abbr      = {AI Open},
}



@inproceedings{DBLP:conf/acl/KeJRCWSZH21,
  author    = {Pei Ke and
               Haozhe Ji and
               Yu Ran and
               Xin Cui and
               Liwei Wang and
               Linfeng Song and
               Xiaoyan Zhu and
               Minlie Huang},
  title     = {JointGT: Graph-Text Joint Representation Learning for Text Generation
               from Knowledge Graphs},
  booktitle = {Findings of the Association for Computational Linguistics,},
  year      = {2021},
  abstract  = {Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the em- bedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets},
  link      = {https://aclanthology.org/2021.findings-acl.223/},
  pdf       = {https://aclanthology.org/2021.findings-acl.223.pdf},
  code      = {https://github.com/thu-coai/JointGT},
  abbr      = {{ACL} Findings},
}

@inproceedings{DBLP:conf/emnlp/JiH21,
  author    = {Haozhe Ji and
               Minlie Huang},
  title     = {DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational
               Transformer},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural
               Language Processing,},
  year      = {2021},
  abstract  = {Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.},
  link      = {https://aclanthology.org/2021.emnlp-main.347/},
  pdf       = {https://aclanthology.org/2021.emnlp-main.347.pdf},
  code      = {https://github.com/cdjhz/discodvt},
  abbr      = {EMNLP}
}